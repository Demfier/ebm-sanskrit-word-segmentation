{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index:\n",
    "This ipynb contains three main parts:\n",
    "1. Data cleaning and data loading\n",
    "2. Seq2Seq model implemented on data using Tensorflow\n",
    "3. Evaluation of the model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_lines = open('input_refined.txt').readlines()\n",
    "output_lines = open('output_refined.txt').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "input_lines = [x.replace('\\n', '') for x in input_lines]\n",
    "output_lines = [x.replace('\\n', '') for x in output_lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['putraM vaMSakaraM rAma jagrAha nfpasaMniDO',\n",
       " 'tato droRo BfSaM krudDo DarmarAjasya saMyuge',\n",
       " \"tatra vamanasyADo gatirUrDvaM virecanasyeti pfTak sAmAnyamuBayoH  sAvaSezOzaDatvaM jIrROzaDatvaM hInadozApahftatvaM vAtaSUlam ayogo 'tiyogo jIvAdAnam ADmAnaM parikartikA parisrAvaH  pravAhikA hfdayopasaraRaM vibanDo 'Ngapragraha iti\",\n",
       " 'grAmyaM na piRqISUrAdiM na qitTAdimapArTakam',\n",
       " 'yaTAdozocCrayaM tasya viSudDasya yaTAkramam',\n",
       " 'aTAnantaram Agatya saMBogamfditAmbaraH',\n",
       " \"taTA svapne'pi\",\n",
       " 'iti',\n",
       " 'karRAtmajaM SaravrAtEScakruScAdfSyam aYjasA',\n",
       " 'evaM kfte na te dozo Bavizyati viSAM pate']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_lines[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['putra vaMSa kara rAma grah nfpa saMniDi',\n",
       " 'tatas droRa BfSam kruD DarmarAja saMyuga',\n",
       " 'tatra vamana aDas gati UrDvam virecana iti pfTak sAmAnya uBaya sa avaSeza OzaDa tva jf OzaDa tva hA doza apahf tva vAta SUla ayoga atiyoga jIvAdAna ADmAna parikartikA parisrAva pravAhikA hfdaya upasaraRa vibanDa aNga pragraha iti',\n",
       " 'grAmya na piRqISUra Adi na qitTa Adi apArTaka',\n",
       " 'yaTA doza ucCraya tad viSuD yaTAkramam',\n",
       " 'aTa anantaram Agam samBoga mfd ambara',\n",
       " 'taTA svapna api',\n",
       " 'iti',\n",
       " 'karRa Atmaja Sara vrAta kf ca adfSya aYjasA',\n",
       " 'evam kf na tvad doza BU viS pati']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_lines[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49998"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49998"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "output_lines = [x for x, y in zip(output_lines, input_lines) if re.match(\"^[A-Za-z0-9_-]*$\", y.replace(\" \", \"\").replace(\"'\", \"\"))]\n",
    "input_lines = [x for x in input_lines if re.match(\"^[A-Za-z0-9_-]*$\", x.replace(\" \", \"\").replace(\"'\", \"\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49873"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49873"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_lines = [x for x, y in zip(input_lines, output_lines) if re.match(\"^[A-Za-z0-9_-]*$\", y.replace(\" \", \"\").replace(\"'\", \"\"))]\n",
    "output_lines = [x for x in output_lines if re.match(\"^[A-Za-z0-9_-]*$\", x.replace(\" \", \"\").replace(\"'\", \"\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49826"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49826"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get dictionaries to convert between indexes and letters/phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_chars = set()\n",
    "for line in input_lines:\n",
    "    for letter in line:\n",
    "        input_chars.add(letter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " \"'\",\n",
       " '1',\n",
       " '0',\n",
       " '3',\n",
       " '2',\n",
       " '5',\n",
       " '4',\n",
       " '7',\n",
       " 'A',\n",
       " 'C',\n",
       " 'B',\n",
       " 'E',\n",
       " 'D',\n",
       " 'G',\n",
       " 'I',\n",
       " 'H',\n",
       " 'K',\n",
       " 'J',\n",
       " 'M',\n",
       " 'O',\n",
       " 'N',\n",
       " 'Q',\n",
       " 'P',\n",
       " 'S',\n",
       " 'R',\n",
       " 'U',\n",
       " 'T',\n",
       " 'W',\n",
       " 'Y',\n",
       " 'a',\n",
       " 'c',\n",
       " 'b',\n",
       " 'e',\n",
       " 'd',\n",
       " 'g',\n",
       " 'f',\n",
       " 'i',\n",
       " 'h',\n",
       " 'k',\n",
       " 'j',\n",
       " 'm',\n",
       " 'l',\n",
       " 'o',\n",
       " 'n',\n",
       " 'q',\n",
       " 'p',\n",
       " 's',\n",
       " 'r',\n",
       " 'u',\n",
       " 't',\n",
       " 'w',\n",
       " 'v',\n",
       " 'y',\n",
       " 'z']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(input_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_chars = set()\n",
    "for line in output_lines:\n",
    "    for letter in line:\n",
    "        output_chars.add(letter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " 'A',\n",
       " 'C',\n",
       " 'B',\n",
       " 'E',\n",
       " 'D',\n",
       " 'G',\n",
       " 'I',\n",
       " 'H',\n",
       " 'K',\n",
       " 'J',\n",
       " 'M',\n",
       " 'O',\n",
       " 'N',\n",
       " 'Q',\n",
       " 'P',\n",
       " 'S',\n",
       " 'R',\n",
       " 'U',\n",
       " 'T',\n",
       " 'W',\n",
       " 'Y',\n",
       " 'a',\n",
       " 'c',\n",
       " 'b',\n",
       " 'e',\n",
       " 'd',\n",
       " 'g',\n",
       " 'f',\n",
       " 'i',\n",
       " 'h',\n",
       " 'k',\n",
       " 'j',\n",
       " 'm',\n",
       " 'l',\n",
       " 'o',\n",
       " 'n',\n",
       " 'q',\n",
       " 'p',\n",
       " 's',\n",
       " 'r',\n",
       " 'u',\n",
       " 't',\n",
       " 'w',\n",
       " 'v',\n",
       " 'y',\n",
       " 'z']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(output_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789 '_\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.lowercase + string.uppercase + string.digits + ' ' + \"'\" + '_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char_set = string.lowercase + string.uppercase + string.digits + ' ' + \"'\" + '_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(char_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index_to_letter = dict(enumerate(char_set))\n",
    "letter_to_index = dict((v, k) for k,v in index_to_letter.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Biggest word in dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49826"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43981"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([x for x in input_lines if len(x) < 50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48679"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([x for x in input_lines if len(x) < 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49826"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45005"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([x for x in output_lines if len(x) < 50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48829"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([x for x in output_lines if len(x) < 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44225"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([x for x, y in zip(input_lines, output_lines) if (len(x) <= 50 and len(y) <= 50)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44225"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([x for x, y in zip(output_lines, input_lines) if (len(x) <= 50 and len(y) <= 50)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_lines_temp = [x for x, y in zip(input_lines, output_lines) if (len(x) <= 50 and len(y) <= 50)]\n",
    "output_lines_temp = [x for x, y in zip(output_lines, input_lines) if (len(x) <= 50 and len(y) <= 50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_lines = input_lines_temp\n",
    "output_lines = output_lines_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44225"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44225"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get rid of words that are too long, or that have punctuation or spaces in them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "c = list(zip(input_lines, output_lines))\n",
    "random.shuffle(c)\n",
    "input_lines, output_lines = zip(*c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "input_ = np.zeros((len(input_lines), 50))\n",
    "labels_ = np.zeros((len(output_lines), 50))\n",
    "\n",
    "for i, (inp, out) in enumerate(zip(input_lines, output_lines)):\n",
    "    inp = inp + \"_\" * (50 - len(inp))\n",
    "    out = out + \"_\" * (50 - len(out))\n",
    "    \n",
    "    for j, letter in enumerate(inp):\n",
    "        input_[i][j] = letter_to_index[letter]\n",
    "    for j, letter in enumerate(out):\n",
    "        labels_[i][j] = letter_to_index[letter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44225, 50)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_ = input_.astype(np.int32)\n",
    "labels_ = labels_.astype(np.int32)\n",
    "    \n",
    "input_test   = input_[:3000]\n",
    "input_val    = input_[3000:6000]\n",
    "input_train  = input_[6000:]\n",
    "labels_test  = labels_[:3000]\n",
    "labels_val   = labels_[3000:6000]\n",
    "labels_train = labels_[6000:]\n",
    "\n",
    "data_test  = zip(input_test, labels_test)\n",
    "data_val   = zip(input_val, labels_val)\n",
    "data_train = zip(input_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Seq2Seq model for sanskrit segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import rnn_cell, seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell resets the graphs and session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ops.reset_default_graph()\n",
    "try:\n",
    "    sess.close()\n",
    "except:\n",
    "    \n",
    "    pass\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_seq_length = 50\n",
    "output_seq_length = 50\n",
    "batch_size = 128\n",
    "\n",
    "input_vocab_size = 65\n",
    "output_vocab_size = 65\n",
    "embedding_dim = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As on this page we take our Seq2Seq learner to have the follwing shape:\n",
    "\n",
    "![alt text](https://www.tensorflow.org/versions/r0.7/images/basic_seq2seq.png \"Seq2Seq\")\n",
    "\n",
    "This means the decode_input has to be shifted along by one from the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encode_input = [tf.placeholder(tf.int32, \n",
    "                                shape=(None,),\n",
    "                                name = \"ei_%i\" %i)\n",
    "                                for i in range(input_seq_length)]\n",
    "\n",
    "labels = [tf.placeholder(tf.int32,\n",
    "                                shape=(None,),\n",
    "                                name = \"l_%i\" %i)\n",
    "                                for i in range(output_seq_length)]\n",
    "\n",
    "decode_input = [tf.zeros_like(encode_input[0], dtype=np.int32, name=\"GO\")] + labels[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell is the meat of the model, and a lot is happening here under the hood.  We take our cells to be LSTM recurrent units, with dropout between the feed-forward layers.  We take 3 of these stacked as our neural network.  We then run this using the seq2seq.embedding_rnn_seq2seq pattern - this let's us hand the neural network sequences like 1,2,3,2,1 - and the neural network automatically embeds this as a one-hot tensor for us.  \n",
    "\n",
    "Note that we build two networks within the 'decoders' scope.  One of these is using feed_previous = True, the other not.  We set this to False during training, so that even if the learner makes a mistake on a letter - we still give it the correct label in the decoder_inputs.  Since we don't have the real label for the test set, this is set to True, and the decoder takes the letter with maximum probability from the last step of the decoder output.  \n",
    "\n",
    "The decode_output is a tensor of shape (batch_size, output_vocab_size).  We can run softmax on this to get logit scores for each letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable decoders/embedding_rnn_seq2seq/RNN/EmbeddingWrapper/embedding already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"<ipython-input-55-58970a8dfcce>\", line 11, in <module>\n    encode_input, decode_input, stacked_lstm, input_vocab_size, output_vocab_size, 1)\n  File \"/home/gaurav/python-scripts/py/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 3066, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/home/gaurav/python-scripts/py/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_ast_nodes\n    if self.run_code(code, result):\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-58970a8dfcce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"decoders\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mscope\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     decode_outputs, decode_state = seq2seq.embedding_rnn_seq2seq(\n\u001b[1;32m---> 11\u001b[1;33m         encode_input, decode_input, stacked_lstm, input_vocab_size, output_vocab_size, 1)\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mscope\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreuse_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/gaurav/python-scripts/py/local/lib/python2.7/site-packages/tensorflow/python/ops/seq2seq.pyc\u001b[0m in \u001b[0;36membedding_rnn_seq2seq\u001b[1;34m(encoder_inputs, decoder_inputs, cell, num_encoder_symbols, num_decoder_symbols, embedding_size, output_projection, feed_previous, dtype, scope)\u001b[0m\n\u001b[0;32m    341\u001b[0m         \u001b[0mcell\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_encoder_symbols\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m         embedding_size=embedding_size)\n\u001b[1;32m--> 343\u001b[1;33m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder_cell\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m     \u001b[1;31m# Decoder.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/gaurav/python-scripts/py/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.pyc\u001b[0m in \u001b[0;36mrnn\u001b[1;34m(cell, inputs, initial_state, dtype, sequence_length, scope)\u001b[0m\n\u001b[0;32m    224\u001b[0m             state_size=cell.state_size)\n\u001b[0;32m    225\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 226\u001b[1;33m         \u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_cell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m       \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/gaurav/python-scripts/py/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    211\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mtime\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mvarscope\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreuse_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m       \u001b[1;31m# pylint: disable=cell-var-from-loop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m       \u001b[0mcall_cell\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m       \u001b[1;31m# pylint: enable=cell-var-from-loop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0msequence_length\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/gaurav/python-scripts/py/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, state, scope)\u001b[0m\n\u001b[0;32m    751\u001b[0m             \u001b[1;34m\"embedding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_embedding_classes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_embedding_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    752\u001b[0m             \u001b[0minitializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 753\u001b[1;33m             dtype=data_type)\n\u001b[0m\u001b[0;32m    754\u001b[0m         embedded = embedding_ops.embedding_lookup(\n\u001b[0;32m    755\u001b[0m             embedding, array_ops.reshape(inputs, [-1]))\n",
      "\u001b[1;32m/home/gaurav/python-scripts/py/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\u001b[0m\n\u001b[0;32m   1022\u001b[0m       \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1024\u001b[1;33m       custom_getter=custom_getter)\n\u001b[0m\u001b[0;32m   1025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1026\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/gaurav/python-scripts/py/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, var_store, name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\u001b[0m\n\u001b[0;32m    848\u001b[0m           \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    849\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 850\u001b[1;33m           custom_getter=custom_getter)\n\u001b[0m\u001b[0;32m    851\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    852\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[1;32m/home/gaurav/python-scripts/py/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\u001b[0m\n\u001b[0;32m    344\u001b[0m           \u001b[0mreuse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m           validate_shape=validate_shape)\n\u001b[0m\u001b[0;32m    347\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[1;32m/home/gaurav/python-scripts/py/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36m_true_getter\u001b[1;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape)\u001b[0m\n\u001b[0;32m    329\u001b[0m           \u001b[0minitializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregularizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mregularizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 331\u001b[1;33m           caching_device=caching_device, validate_shape=validate_shape)\n\u001b[0m\u001b[0;32m    332\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/gaurav/python-scripts/py/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36m_get_single_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape)\u001b[0m\n\u001b[0;32m    630\u001b[0m                          \u001b[1;34m\" Did you mean to set reuse=True in VarScope? \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    631\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[1;32m--> 632\u001b[1;33m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[0;32m    633\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Variable decoders/embedding_rnn_seq2seq/RNN/EmbeddingWrapper/embedding already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"<ipython-input-55-58970a8dfcce>\", line 11, in <module>\n    encode_input, decode_input, stacked_lstm, input_vocab_size, output_vocab_size, 1)\n  File \"/home/gaurav/python-scripts/py/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 3066, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/home/gaurav/python-scripts/py/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_ast_nodes\n    if self.run_code(code, result):\n"
     ]
    }
   ],
   "source": [
    "keep_prob = tf.placeholder(\"float\")\n",
    "\n",
    "cells = [rnn_cell.DropoutWrapper(\n",
    "        rnn_cell.BasicLSTMCell(embedding_dim), output_keep_prob=keep_prob\n",
    "    ) for i in range(3)]\n",
    "\n",
    "stacked_lstm = rnn_cell.MultiRNNCell(cells)\n",
    "\n",
    "with tf.variable_scope(\"decoders\") as scope:\n",
    "    decode_outputs, decode_state = seq2seq.embedding_rnn_seq2seq(\n",
    "        encode_input, decode_input, stacked_lstm, input_vocab_size, output_vocab_size, 1)\n",
    "    \n",
    "    scope.reuse_variables()\n",
    "    \n",
    "    decode_outputs_test, decode_state_test = seq2seq.embedding_rnn_seq2seq(\n",
    "        encode_input, decode_input, stacked_lstm, input_vocab_sizeab_size, output_vocab_size, 1, \n",
    "    feed_previous=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sequence_loss is cross-entropy on the soft max of the decode outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'decode_outputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-2487182dfee0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mloss_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mseq2seq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequence_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecode_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_vocab_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1e-4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtrain_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'decode_outputs' is not defined"
     ]
    }
   ],
   "source": [
    "loss_weights = [tf.ones_like(l, dtype=tf.float32) for l in labels]\n",
    "loss = seq2seq.sequence_loss(decode_outputs, labels, loss_weights, output_vocab_size)\n",
    "optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple class for getting random batches and reshaping them properly for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataIterator:\n",
    "    def __init__(self, data, batch_size):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.iter = self.make_random_iter()\n",
    "        \n",
    "    def next_batch(self):\n",
    "        try:\n",
    "            idxs = self.iter.next()\n",
    "        except StopIteration:\n",
    "            self.iter = self.make_random_iter()\n",
    "            idxs = self.iter.next()\n",
    "        X, Y = zip(*[self.data[i] for i in idxs])\n",
    "        X = np.array(X).T\n",
    "        Y = np.array(Y).T\n",
    "        return X, Y\n",
    "\n",
    "    def make_random_iter(self):\n",
    "        splits = np.arange(self.batch_size, len(self.data), self.batch_size)\n",
    "        it = np.split(np.random.permutation(range(len(self.data))), splits)[:-1]\n",
    "        return iter(it)\n",
    "    \n",
    "train_iter = DataIterator(data_train, 128)\n",
    "val_iter = DataIterator(data_val, 128)\n",
    "test_iter = DataIterator(data_test, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our evaluation scores are based on the seq2seq loss, and on the precision - the number of words that the model spells perfectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def get_feed(X, Y):\n",
    "    feed_dict = {encode_input[t]: X[t] for t in range(input_seq_length)}\n",
    "    feed_dict.update({labels[t]: Y[t] for t in range(output_seq_length)})\n",
    "    return feed_dict\n",
    "\n",
    "def train_batch(data_iter):\n",
    "    X, Y = data_iter.next_batch()\n",
    "    feed_dict = get_feed(X, Y)\n",
    "    feed_dict[keep_prob] = 0.5\n",
    "    _, out = sess.run([train_op, loss], feed_dict)\n",
    "    return out\n",
    "\n",
    "def get_eval_batch_data(data_iter):\n",
    "    X, Y = data_iter.next_batch()\n",
    "    feed_dict = get_feed(X, Y)\n",
    "    feed_dict[keep_prob] = 1.\n",
    "    all_output = sess.run([loss] + decode_outputs_test, feed_dict)\n",
    "    eval_loss = all_output[0]\n",
    "    decode_output = np.array(all_output[1:]).transpose([1,0,2])\n",
    "    return eval_loss, decode_output, X, Y\n",
    "\n",
    "def eval_batch(data_iter, num_batches):\n",
    "    losses = []\n",
    "    predict_loss = []\n",
    "    for i in range(num_batches):\n",
    "        eval_loss, output, X, Y = get_eval_batch_data(data_iter)\n",
    "        losses.append(eval_loss)\n",
    "        \n",
    "        for index in range(len(output)):\n",
    "            real = Y.T[index]\n",
    "            predict = np.argmax(output, axis = 2)[index]\n",
    "            predict_loss.append(all(real==predict))\n",
    "    return np.mean(losses), np.mean(predict_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#saver.restore(sess, \"skt.ckpt\") #if pretrained model is present => load it\n",
    "saver = tf.train.Saver()\n",
    "for i in range(10):\n",
    "    try:\n",
    "        train_batch(train_iter)\n",
    "        if i % 1000 == 0:\n",
    "            val_loss, val_predict = eval_batch(val_iter, 16)\n",
    "            train_loss, train_predict = eval_batch(train_iter, 16)\n",
    "            print \"val loss   : %f, val predict   = %.1f%%\" %(val_loss, val_predict * 100)\n",
    "            print \"train loss : %f, train predict = %.1f%%\" %(train_loss, train_predict * 100)\n",
    "            print\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "            saver.save(sess, \"skt.ckpt\") #Saving the model to skt.ckpt file\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print \"interrupted by user\"\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Examining model outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "saver.restore(sess, \"skt.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eval_loss, output, X, Y = get_eval_batch_data(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index in random.sample(range(len(output)), 10):\n",
    "    inp = [index_to_letter[l] for l in X.T[index]] \n",
    "    real = [index_to_letter[l] for l in Y.T[index]] \n",
    "    predict = [index_to_letter[l] for l in np.argmax(output, axis = 2)[index]]\n",
    "    \n",
    "    print \"input :        \" + \"\".join(inp).split(\"_\")[0]\n",
    "    print \"real output :  \" + \"\".join(real).split(\"_\")[0]\n",
    "    print \"model output : \" + \"\".join(predict).split(\"_\")[0]\n",
    "    print \"is correct :   \" + str(real == predict)\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for getting outputs that are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for index in range(len(output)):\n",
    "    inp = [index_to_letter[l] for l in X.T[index]] \n",
    "    real = [index_to_letter[l] for l in Y.T[index]] \n",
    "    predict = [index_to_letter[l] for l in np.argmax(output, axis = 2)[index]]\n",
    "    \n",
    "    if (real != predict):\n",
    "        print \"input :        \" + \"\".join(inp).split(\"_\")[0]\n",
    "        print \"real output :  \" + \"\".join(real).split(\"_\")[0]\n",
    "        print \"model output : \" + \"\".join(predict).split(\"_\")[0]\n",
    "        print \"is correct :   \" + str(real == predict)\n",
    "        print "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
